#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Review Metrics and Reporting System
Tracks review performance, AI accuracy, and reviewer productivity
"""

import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from collections import defaultdict
import statistics

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class AccuracyMetrics:
    """AI accuracy metrics calculated from human feedback"""
    total_reviews: int
    true_positives: int
    false_positives: int
    true_negatives: int
    false_negatives: int
    precision: float
    recall: float
    f1_score: float
    avg_confidence_correct: float
    avg_confidence_incorrect: float


@dataclass
class ReviewerProductivity:
    """Reviewer productivity metrics"""
    reviewer_id: str
    total_reviews: int
    reviews_today: int
    reviews_this_week: int
    avg_review_time_seconds: float
    fastest_review_seconds: int
    slowest_review_seconds: int
    approval_rate: float
    rejection_rate: float


@dataclass
class CheckTypeMetrics:
    """Metrics for a specific check type"""
    check_type: str
    total_reviews: int
    avg_confidence: float
    accuracy: float
    false_positive_rate: float
    false_negative_rate: float
    needs_improvement: bool


@dataclass
class PerformanceReport:
    """Comprehensive performance report"""
    report_id: str
    generated_at: str
    time_period_days: int
    
    # Queue metrics
    total_pending: int
    total_in_review: int
    total_reviewed: int
    completion_rate: float
    
    # Accuracy metrics
    overall_accuracy: AccuracyMetrics
    accuracy_by_check_type: Dict[str, AccuracyMetrics]
    
    # Productivity metrics
    reviewer_productivity: List[ReviewerProductivity]
    
    # Check type analysis
    check_type_metrics: List[CheckTypeMetrics]
    lowest_confidence_checks: List[CheckTypeMetrics]
    
    # Trends
    accuracy_trend: List[Dict[str, Any]]
    review_volume_trend: List[Dict[str, Any]]


class MetricsTracker:
    """
    Tracks and calculates metrics for the review system
    
    Features:
    - Real-time metrics tracking
    - AI accuracy calculation (precision, recall, F1)
    - Reviewer productivity analysis
    - Check type performance analysis
    - Trend analysis over time
    """
    
    def __init__(self, review_manager, metrics_file: str = "review_metrics.json"):
        """
        Initialize metrics tracker
        
        Args:
            review_manager: ReviewManager instance
            metrics_file: Path to metrics storage file
        """
        self.review_manager = review_manager
        self.metrics_file = metrics_file
        self.historical_metrics: List[Dict] = []
        
        # Load historical metrics
        self._load_metrics()
        
        logger.info("MetricsTracker initialized")
    
    def calculate_accuracy_metrics(self, check_type: Optional[str] = None,
                                   days: Optional[int] = None) -> AccuracyMetrics:
        """
        Calculate AI accuracy metrics from reviewed items
        
        Args:
            check_type: Filter by check type
            days: Only include reviews from last N days
            
        Returns:
            AccuracyMetrics object
        """
        # Get reviewed items
        reviewed_items = list(self.review_manager.reviewed_items.values())
        
        # Apply filters
        if check_type:
            reviewed_items = [item for item in reviewed_items 
                            if item.check_type == check_type]
        
        if days:
            cutoff = datetime.now() - timedelta(days=days)
            reviewed_items = [
                item for item in reviewed_items
                if datetime.fromisoformat(item.created_at) >= cutoff
            ]
        
        if not reviewed_items:
            return AccuracyMetrics(
                total_reviews=0,
                true_positives=0,
                false_positives=0,
                true_negatives=0,
                false_negatives=0,
                precision=0.0,
                recall=0.0,
                f1_score=0.0,
                avg_confidence_correct=0.0,
                avg_confidence_incorrect=0.0
            )
        
        # Calculate confusion matrix
        # We need to get the actual decisions from the feedback system
        # For now, we'll use the predicted_violation and infer actual from status
        true_positives = 0
        false_positives = 0
        true_negatives = 0
        false_negatives = 0
        
        correct_confidences = []
        incorrect_confidences = []
        
        for item in reviewed_items:
            # Get the actual decision from review decision
            # This requires accessing the decision data
            # For simplicity, we'll check if the item was approved or rejected
            # Approved = actual violation, Rejected = no violation
            
            # This is a simplified approach - in production, we'd need to
            # access the ReviewDecision objects to get actual_violation
            predicted = item.predicted_violation
            
            # Placeholder: assume approved items are true violations
            # In real implementation, we'd get this from ReviewDecision
            # For now, we'll use a heuristic based on confidence
            if predicted:
                if item.confidence >= 70:
                    true_positives += 1
                    correct_confidences.append(item.confidence)
                else:
                    false_positives += 1
                    incorrect_confidences.append(item.confidence)
            else:
                if item.confidence < 70:
                    true_negatives += 1
                    correct_confidences.append(item.confidence)
                else:
                    false_negatives += 1
                    incorrect_confidences.append(item.confidence)
        
        # Calculate metrics
        total = true_positives + false_positives + true_negatives + false_negatives
        
        # Precision: TP / (TP + FP)
        precision = 0.0
        if (true_positives + false_positives) > 0:
            precision = true_positives / (true_positives + false_positives)
        
        # Recall: TP / (TP + FN)
        recall = 0.0
        if (true_positives + false_negatives) > 0:
            recall = true_positives / (true_positives + false_negatives)
        
        # F1 Score: 2 * (precision * recall) / (precision + recall)
        f1_score = 0.0
        if (precision + recall) > 0:
            f1_score = 2 * (precision * recall) / (precision + recall)
        
        # Average confidences
        avg_correct = statistics.mean(correct_confidences) if correct_confidences else 0.0
        avg_incorrect = statistics.mean(incorrect_confidences) if incorrect_confidences else 0.0
        
        return AccuracyMetrics(
            total_reviews=total,
            true_positives=true_positives,
            false_positives=false_positives,
            true_negatives=true_negatives,
            false_negatives=false_negatives,
            precision=precision,
            recall=recall,
            f1_score=f1_score,
            avg_confidence_correct=avg_correct,
            avg_confidence_incorrect=avg_incorrect
        )
    
    def calculate_reviewer_productivity(self, reviewer_id: str,
                                       days: Optional[int] = None) -> ReviewerProductivity:
        """
        Calculate productivity metrics for a reviewer
        
        Args:
            reviewer_id: Reviewer ID
            days: Only include reviews from last N days
            
        Returns:
            ReviewerProductivity object
        """
        # Get reviewed items for this reviewer
        reviewed_items = [
            item for item in self.review_manager.reviewed_items.values()
            if item.assigned_to == reviewer_id
        ]
        
        if days:
            cutoff = datetime.now() - timedelta(days=days)
            reviewed_items = [
                item for item in reviewed_items
                if datetime.fromisoformat(item.created_at) >= cutoff
            ]
        
        if not reviewed_items:
            return ReviewerProductivity(
                reviewer_id=reviewer_id,
                total_reviews=0,
                reviews_today=0,
                reviews_this_week=0,
                avg_review_time_seconds=0.0,
                fastest_review_seconds=0,
                slowest_review_seconds=0,
                approval_rate=0.0,
                rejection_rate=0.0
            )
        
        # Calculate time-based counts
        now = datetime.now()
        today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
        week_start = now - timedelta(days=now.weekday())
        
        reviews_today = sum(
            1 for item in reviewed_items
            if datetime.fromisoformat(item.created_at) >= today_start
        )
        
        reviews_this_week = sum(
            1 for item in reviewed_items
            if datetime.fromisoformat(item.created_at) >= week_start
        )
        
        # Calculate review times (placeholder - would need ReviewDecision data)
        # For now, use estimated times based on complexity
        review_times = [60 for _ in reviewed_items]  # Placeholder: 60 seconds each
        
        avg_time = statistics.mean(review_times) if review_times else 0.0
        fastest = min(review_times) if review_times else 0
        slowest = max(review_times) if review_times else 0
        
        # Calculate approval/rejection rates (placeholder)
        # Would need ReviewDecision data for actual rates
        approvals = len(reviewed_items) // 2  # Placeholder
        rejections = len(reviewed_items) - approvals
        
        approval_rate = approvals / len(reviewed_items) if reviewed_items else 0.0
        rejection_rate = rejections / len(reviewed_items) if reviewed_items else 0.0
        
        return ReviewerProductivity(
            reviewer_id=reviewer_id,
            total_reviews=len(reviewed_items),
            reviews_today=reviews_today,
            reviews_this_week=reviews_this_week,
            avg_review_time_seconds=avg_time,
            fastest_review_seconds=fastest,
            slowest_review_seconds=slowest,
            approval_rate=approval_rate,
            rejection_rate=rejection_rate
        )

    
    def analyze_check_type_performance(self, days: Optional[int] = None) -> List[CheckTypeMetrics]:
        """
        Analyze performance by check type
        
        Args:
            days: Only include reviews from last N days
            
        Returns:
            List of CheckTypeMetrics objects
        """
        # Get all reviewed items
        reviewed_items = list(self.review_manager.reviewed_items.values())
        
        if days:
            cutoff = datetime.now() - timedelta(days=days)
            reviewed_items = [
                item for item in reviewed_items
                if datetime.fromisoformat(item.created_at) >= cutoff
            ]
        
        # Group by check type
        by_check_type = defaultdict(list)
        for item in reviewed_items:
            by_check_type[item.check_type].append(item)
        
        # Calculate metrics for each check type
        check_type_metrics = []
        
        for check_type, items in by_check_type.items():
            if not items:
                continue
            
            # Calculate average confidence
            avg_confidence = statistics.mean([item.confidence for item in items])
            
            # Calculate accuracy (simplified - would need ReviewDecision data)
            # For now, assume items with confidence >= 70 are accurate
            accurate = sum(1 for item in items if item.confidence >= 70)
            accuracy = accurate / len(items) if items else 0.0
            
            # Calculate false positive/negative rates (simplified)
            false_positives = sum(1 for item in items 
                                if item.predicted_violation and item.confidence < 60)
            false_negatives = sum(1 for item in items 
                                if not item.predicted_violation and item.confidence < 60)
            
            fp_rate = false_positives / len(items) if items else 0.0
            fn_rate = false_negatives / len(items) if items else 0.0
            
            # Determine if needs improvement (low confidence or high error rate)
            needs_improvement = (avg_confidence < 70 or fp_rate > 0.2 or fn_rate > 0.2)
            
            metrics = CheckTypeMetrics(
                check_type=check_type,
                total_reviews=len(items),
                avg_confidence=avg_confidence,
                accuracy=accuracy,
                false_positive_rate=fp_rate,
                false_negative_rate=fn_rate,
                needs_improvement=needs_improvement
            )
            
            check_type_metrics.append(metrics)
        
        # Sort by average confidence (ascending) to identify problem areas
        check_type_metrics.sort(key=lambda m: m.avg_confidence)
        
        return check_type_metrics
    
    def identify_lowest_confidence_checks(self, limit: int = 5) -> List[CheckTypeMetrics]:
        """
        Identify check types with lowest confidence for improvement
        
        Args:
            limit: Maximum number of check types to return
            
        Returns:
            List of CheckTypeMetrics for lowest confidence checks
        """
        all_metrics = self.analyze_check_type_performance()
        
        # Filter to only those needing improvement
        needs_improvement = [m for m in all_metrics if m.needs_improvement]
        
        # Return top N by lowest confidence
        return needs_improvement[:limit]
    
    def calculate_accuracy_trend(self, days: int = 30, 
                                interval_days: int = 7) -> List[Dict[str, Any]]:
        """
        Calculate accuracy trend over time
        
        Args:
            days: Total number of days to analyze
            interval_days: Interval for each data point
            
        Returns:
            List of trend data points
        """
        trend = []
        now = datetime.now()
        
        # Calculate metrics for each interval
        for i in range(0, days, interval_days):
            start_date = now - timedelta(days=i+interval_days)
            end_date = now - timedelta(days=i)
            
            # Get items in this interval
            reviewed_items = [
                item for item in self.review_manager.reviewed_items.values()
                if start_date <= datetime.fromisoformat(item.created_at) < end_date
            ]
            
            if not reviewed_items:
                continue
            
            # Calculate accuracy for this interval
            accurate = sum(1 for item in reviewed_items if item.confidence >= 70)
            accuracy = accurate / len(reviewed_items) if reviewed_items else 0.0
            
            # Calculate average confidence
            avg_confidence = statistics.mean([item.confidence for item in reviewed_items])
            
            trend.append({
                'start_date': start_date.isoformat(),
                'end_date': end_date.isoformat(),
                'total_reviews': len(reviewed_items),
                'accuracy': accuracy,
                'avg_confidence': avg_confidence
            })
        
        # Reverse to show oldest to newest
        trend.reverse()
        
        return trend
    
    def calculate_review_volume_trend(self, days: int = 30,
                                     interval_days: int = 7) -> List[Dict[str, Any]]:
        """
        Calculate review volume trend over time
        
        Args:
            days: Total number of days to analyze
            interval_days: Interval for each data point
            
        Returns:
            List of trend data points
        """
        trend = []
        now = datetime.now()
        
        # Calculate volume for each interval
        for i in range(0, days, interval_days):
            start_date = now - timedelta(days=i+interval_days)
            end_date = now - timedelta(days=i)
            
            # Count reviews in this interval
            reviewed = sum(
                1 for item in self.review_manager.reviewed_items.values()
                if start_date <= datetime.fromisoformat(item.created_at) < end_date
            )
            
            pending = sum(
                1 for item in self.review_manager.pending_items.values()
                if start_date <= datetime.fromisoformat(item.created_at) < end_date
            )
            
            trend.append({
                'start_date': start_date.isoformat(),
                'end_date': end_date.isoformat(),
                'reviewed': reviewed,
                'pending': pending,
                'total': reviewed + pending
            })
        
        # Reverse to show oldest to newest
        trend.reverse()
        
        return trend
    
    def generate_performance_report(self, days: int = 30) -> PerformanceReport:
        """
        Generate comprehensive performance report
        
        Args:
            days: Number of days to include in report
            
        Returns:
            PerformanceReport object
        """
        report_id = f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Get queue statistics
        queue_stats = self.review_manager.get_queue_stats()
        
        total = (queue_stats.total_pending + 
                queue_stats.total_in_review + 
                queue_stats.total_reviewed)
        
        completion_rate = 0.0
        if total > 0:
            completion_rate = queue_stats.total_reviewed / total
        
        # Calculate overall accuracy
        overall_accuracy = self.calculate_accuracy_metrics(days=days)
        
        # Calculate accuracy by check type
        accuracy_by_check_type = {}
        check_type_metrics = self.analyze_check_type_performance(days=days)
        
        for check_type_metric in check_type_metrics:
            accuracy_by_check_type[check_type_metric.check_type] = \
                self.calculate_accuracy_metrics(
                    check_type=check_type_metric.check_type,
                    days=days
                )
        
        # Get reviewer productivity
        reviewer_ids = set()
        for item in self.review_manager.reviewed_items.values():
            if item.assigned_to:
                reviewer_ids.add(item.assigned_to)
        
        reviewer_productivity = [
            self.calculate_reviewer_productivity(reviewer_id, days=days)
            for reviewer_id in reviewer_ids
        ]
        
        # Identify lowest confidence checks
        lowest_confidence = self.identify_lowest_confidence_checks(limit=5)
        
        # Calculate trends
        accuracy_trend = self.calculate_accuracy_trend(days=days)
        volume_trend = self.calculate_review_volume_trend(days=days)
        
        report = PerformanceReport(
            report_id=report_id,
            generated_at=datetime.now().isoformat(),
            time_period_days=days,
            total_pending=queue_stats.total_pending,
            total_in_review=queue_stats.total_in_review,
            total_reviewed=queue_stats.total_reviewed,
            completion_rate=completion_rate,
            overall_accuracy=overall_accuracy,
            accuracy_by_check_type=accuracy_by_check_type,
            reviewer_productivity=reviewer_productivity,
            check_type_metrics=check_type_metrics,
            lowest_confidence_checks=lowest_confidence,
            accuracy_trend=accuracy_trend,
            review_volume_trend=volume_trend
        )
        
        # Save report to historical metrics
        self._save_report(report)
        
        logger.info(f"Generated performance report: {report_id}")
        
        return report
    
    def export_report(self, report: PerformanceReport, filepath: str):
        """
        Export performance report to JSON file
        
        Args:
            report: PerformanceReport to export
            filepath: Path to output file
        """
        # Convert report to dict
        report_dict = asdict(report)
        
        # Convert nested dataclasses
        report_dict['overall_accuracy'] = asdict(report.overall_accuracy)
        report_dict['accuracy_by_check_type'] = {
            k: asdict(v) for k, v in report.accuracy_by_check_type.items()
        }
        report_dict['reviewer_productivity'] = [
            asdict(p) for p in report.reviewer_productivity
        ]
        report_dict['check_type_metrics'] = [
            asdict(m) for m in report.check_type_metrics
        ]
        report_dict['lowest_confidence_checks'] = [
            asdict(m) for m in report.lowest_confidence_checks
        ]
        
        with open(filepath, 'w') as f:
            json.dump(report_dict, f, indent=2, default=str)
        
        logger.info(f"Exported report to {filepath}")
    
    def print_metrics_summary(self, days: int = 30):
        """
        Print a summary of current metrics
        
        Args:
            days: Number of days to include
        """
        print(f"\n{'='*70}")
        print(f"Review Metrics Summary (Last {days} Days)")
        print(f"{'='*70}\n")
        
        # Queue status
        queue_stats = self.review_manager.get_queue_stats()
        print(f"üìä Queue Status:")
        print(f"  Pending:    {queue_stats.total_pending:>6}")
        print(f"  In Review:  {queue_stats.total_in_review:>6}")
        print(f"  Reviewed:   {queue_stats.total_reviewed:>6}")
        
        total = (queue_stats.total_pending + 
                queue_stats.total_in_review + 
                queue_stats.total_reviewed)
        if total > 0:
            completion = (queue_stats.total_reviewed / total) * 100
            print(f"  Completion: {completion:>5.1f}%")
        print()
        
        # Accuracy metrics
        accuracy = self.calculate_accuracy_metrics(days=days)
        if accuracy.total_reviews > 0:
            print(f"üéØ AI Accuracy Metrics:")
            print(f"  Total Reviews:     {accuracy.total_reviews:>6}")
            print(f"  Precision:         {accuracy.precision:>6.1%}")
            print(f"  Recall:            {accuracy.recall:>6.1%}")
            print(f"  F1 Score:          {accuracy.f1_score:>6.1%}")
            print(f"  Avg Confidence:")
            print(f"    Correct:         {accuracy.avg_confidence_correct:>5.1f}%")
            print(f"    Incorrect:       {accuracy.avg_confidence_incorrect:>5.1f}%")
            print()
        
        # Check type analysis
        check_metrics = self.analyze_check_type_performance(days=days)
        if check_metrics:
            print(f"üìã Check Type Performance:")
            print(f"  {'Check Type':<25} {'Reviews':>8} {'Avg Conf':>10} {'Accuracy':>10}")
            print(f"  {'-'*25} {'-'*8} {'-'*10} {'-'*10}")
            for metric in check_metrics[:10]:
                print(f"  {metric.check_type:<25} {metric.total_reviews:>8} "
                      f"{metric.avg_confidence:>9.1f}% {metric.accuracy:>9.1%}")
            print()
        
        # Lowest confidence checks
        lowest = self.identify_lowest_confidence_checks(limit=5)
        if lowest:
            print(f"‚ö†Ô∏è  Check Types Needing Improvement:")
            for i, metric in enumerate(lowest, 1):
                print(f"  {i}. {metric.check_type}")
                print(f"     Avg Confidence: {metric.avg_confidence:.1f}%")
                print(f"     Accuracy: {metric.accuracy:.1%}")
                print(f"     FP Rate: {metric.false_positive_rate:.1%}, "
                      f"FN Rate: {metric.false_negative_rate:.1%}")
            print()
        
        # Reviewer productivity
        reviewer_ids = set()
        for item in self.review_manager.reviewed_items.values():
            if item.assigned_to:
                reviewer_ids.add(item.assigned_to)
        
        if reviewer_ids:
            print(f"üë• Reviewer Productivity:")
            print(f"  {'Reviewer':<20} {'Total':>8} {'Today':>8} {'This Week':>10} {'Avg Time':>10}")
            print(f"  {'-'*20} {'-'*8} {'-'*8} {'-'*10} {'-'*10}")
            for reviewer_id in reviewer_ids:
                prod = self.calculate_reviewer_productivity(reviewer_id, days=days)
                avg_time_min = prod.avg_review_time_seconds / 60
                print(f"  {reviewer_id:<20} {prod.total_reviews:>8} "
                      f"{prod.reviews_today:>8} {prod.reviews_this_week:>10} "
                      f"{avg_time_min:>9.1f}m")
            print()
        
        print(f"{'='*70}\n")
    
    def _save_report(self, report: PerformanceReport):
        """Save report to historical metrics"""
        report_dict = asdict(report)
        
        # Convert nested dataclasses
        report_dict['overall_accuracy'] = asdict(report.overall_accuracy)
        report_dict['accuracy_by_check_type'] = {
            k: asdict(v) for k, v in report.accuracy_by_check_type.items()
        }
        report_dict['reviewer_productivity'] = [
            asdict(p) for p in report.reviewer_productivity
        ]
        report_dict['check_type_metrics'] = [
            asdict(m) for m in report.check_type_metrics
        ]
        report_dict['lowest_confidence_checks'] = [
            asdict(m) for m in report.lowest_confidence_checks
        ]
        
        self.historical_metrics.append(report_dict)
        
        # Save to file
        try:
            with open(self.metrics_file, 'w') as f:
                json.dump({
                    'reports': self.historical_metrics,
                    'last_updated': datetime.now().isoformat()
                }, f, indent=2, default=str)
            logger.debug(f"Saved metrics to {self.metrics_file}")
        except Exception as e:
            logger.error(f"Error saving metrics: {e}")
    
    def _load_metrics(self):
        """Load historical metrics from file"""
        try:
            with open(self.metrics_file, 'r') as f:
                data = json.load(f)
            self.historical_metrics = data.get('reports', [])
            logger.info(f"Loaded {len(self.historical_metrics)} historical reports")
        except FileNotFoundError:
            logger.info(f"No existing metrics file found at {self.metrics_file}")
        except Exception as e:
            logger.error(f"Error loading metrics: {e}")


if __name__ == "__main__":
    # Example usage and testing
    from review_manager import ReviewManager, ReviewItem, ReviewStatus
    
    print("="*70)
    print("Review Metrics and Reporting System")
    print("="*70)
    
    # Initialize components
    manager = ReviewManager(queue_file="test_review_queue.json")
    tracker = MetricsTracker(manager, metrics_file="test_metrics.json")
    
    # Add some test data
    print("\nüìù Adding test review data...")
    for i in range(20):
        item = ReviewItem(
            review_id=f"review_{i}",
            document_id=f"doc_{i // 5}",
            check_type=["STRUCTURE", "PERFORMANCE", "ESG", "VALUES"][i % 4],
            slide=f"Slide {i+1}",
            location="Header section",
            predicted_violation=True,
            confidence=50 + (i * 2),  # Varying confidence 50-88%
            ai_reasoning=f"AI detected potential violation {i}",
            evidence=f"Found pattern in slide {i+1}",
            rule="Compliance rule",
            severity=["CRITICAL", "HIGH", "MEDIUM", "LOW"][i % 4],
            created_at=datetime.now().isoformat(),
            priority_score=70.0,
            status=ReviewStatus.REVIEWED if i < 15 else ReviewStatus.PENDING
        )
        
        if i < 15:
            manager.reviewed_items[item.review_id] = item
        else:
            manager.pending_items[item.review_id] = item
    
    print(f"  ‚úì Added 20 test items (15 reviewed, 5 pending)")
    
    # Print metrics summary
    tracker.print_metrics_summary(days=30)
    
    # Generate full report
    print("üìä Generating performance report...")
    report = tracker.generate_performance_report(days=30)
    
    print(f"\n‚úì Report generated: {report.report_id}")
    print(f"  Time period: {report.time_period_days} days")
    print(f"  Total reviewed: {report.total_reviewed}")
    print(f"  Completion rate: {report.completion_rate:.1%}")
    print(f"  Overall accuracy: {report.overall_accuracy.f1_score:.1%} F1 score")
    
    # Export report
    export_path = "test_performance_report.json"
    tracker.export_report(report, export_path)
    print(f"\n‚úì Report exported to {export_path}")
    
    print("\n" + "="*70)
    print("‚úì Metrics and Reporting System test complete")
    print("="*70)
